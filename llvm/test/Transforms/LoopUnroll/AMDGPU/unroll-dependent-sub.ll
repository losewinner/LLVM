; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; REQUIRES: asserts
; RUN: opt -S -mtriple=amdgcn-- -passes=loop-unroll -debug < %s 2>&1 | FileCheck %s

; For @dependent_sub_fullunroll, the threshold bonus should apply
; CHECK: due to subloop's trip count becoming runtime-independent after unrolling

; For @dependent_sub_no_fullunroll, the threshold bonus should not apply
; CHECK-NOT: due to subloop's trip count becoming runtime-independent after unrolling

; For @dont_unroll_illegal_convergent_op, the threshold bonus should apply even if there is no unrolling
; CHECK: due to subloop's trip count becoming runtime-independent after unrolling

; Check that the outer loop of a double-nested loop where the inner loop's trip
; count depends exclusively on constants and the outer IV is fully unrolled
; thanks to receiving a threshold bonus in AMDGPU's TTI.

define void @dependent_sub_fullunroll(ptr noundef %mem) {
; CHECK-LABEL: @dependent_sub_fullunroll(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[OUTER_HEADER:%.*]]
; CHECK:       outer.header:
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING:%.*]]
; CHECK:       inner.header_latch_exiting:
; CHECK-NEXT:    [[INNER_IV:%.*]] = phi i32 [ 0, [[OUTER_HEADER]] ], [ [[INNER_IV_NEXT:%.*]], [[INNER_HEADER_LATCH_EXITING]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT]] = add nuw nsw i32 [[INNER_IV]], 1
; CHECK-NEXT:    [[INNER_IV_EXT:%.*]] = zext nneg i32 [[INNER_IV]] to i64
; CHECK-NEXT:    [[ADDR:%.*]] = getelementptr inbounds i8, ptr [[MEM:%.*]], i64 [[INNER_IV_EXT]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR]], align 4
; CHECK-NEXT:    [[INNER_COND:%.*]] = icmp ult i32 [[INNER_IV_NEXT]], 8
; CHECK-NEXT:    br i1 [[INNER_COND]], label [[INNER_HEADER_LATCH_EXITING]], label [[OUTER_LATCH_EXITING:%.*]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       outer.latch_exiting:
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING_1:%.*]]
; CHECK:       inner.header_latch_exiting.1:
; CHECK-NEXT:    [[INNER_IV_1:%.*]] = phi i32 [ 1, [[OUTER_LATCH_EXITING]] ], [ [[INNER_IV_NEXT_1:%.*]], [[INNER_HEADER_LATCH_EXITING_1]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT_1]] = add nuw nsw i32 [[INNER_IV_1]], 1
; CHECK-NEXT:    [[INNER_IV_EXT_1:%.*]] = zext nneg i32 [[INNER_IV_1]] to i64
; CHECK-NEXT:    [[IDX_1:%.*]] = add nuw nsw i64 16, [[INNER_IV_EXT_1]]
; CHECK-NEXT:    [[ADDR_1:%.*]] = getelementptr inbounds i8, ptr [[MEM]], i64 [[IDX_1]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR_1]], align 4
; CHECK-NEXT:    [[INNER_COND_1:%.*]] = icmp ult i32 [[INNER_IV_NEXT_1]], 8
; CHECK-NEXT:    br i1 [[INNER_COND_1]], label [[INNER_HEADER_LATCH_EXITING_1]], label [[OUTER_LATCH_EXITING_1:%.*]], !llvm.loop [[LOOP0]]
; CHECK:       outer.latch_exiting.1:
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING_2:%.*]]
; CHECK:       inner.header_latch_exiting.2:
; CHECK-NEXT:    [[INNER_IV_2:%.*]] = phi i32 [ 2, [[OUTER_LATCH_EXITING_1]] ], [ [[INNER_IV_NEXT_2:%.*]], [[INNER_HEADER_LATCH_EXITING_2]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT_2]] = add nuw nsw i32 [[INNER_IV_2]], 1
; CHECK-NEXT:    [[INNER_IV_EXT_2:%.*]] = zext nneg i32 [[INNER_IV_2]] to i64
; CHECK-NEXT:    [[IDX_2:%.*]] = add nuw nsw i64 32, [[INNER_IV_EXT_2]]
; CHECK-NEXT:    [[ADDR_2:%.*]] = getelementptr inbounds i8, ptr [[MEM]], i64 [[IDX_2]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR_2]], align 4
; CHECK-NEXT:    [[INNER_COND_2:%.*]] = icmp ult i32 [[INNER_IV_NEXT_2]], 8
; CHECK-NEXT:    br i1 [[INNER_COND_2]], label [[INNER_HEADER_LATCH_EXITING_2]], label [[OUTER_LATCH_EXITING_2:%.*]], !llvm.loop [[LOOP0]]
; CHECK:       outer.latch_exiting.2:
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING_3:%.*]]
; CHECK:       inner.header_latch_exiting.3:
; CHECK-NEXT:    [[INNER_IV_3:%.*]] = phi i32 [ 3, [[OUTER_LATCH_EXITING_2]] ], [ [[INNER_IV_NEXT_3:%.*]], [[INNER_HEADER_LATCH_EXITING_3]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT_3]] = add nuw nsw i32 [[INNER_IV_3]], 1
; CHECK-NEXT:    [[INNER_IV_EXT_3:%.*]] = zext nneg i32 [[INNER_IV_3]] to i64
; CHECK-NEXT:    [[IDX_3:%.*]] = add nuw nsw i64 48, [[INNER_IV_EXT_3]]
; CHECK-NEXT:    [[ADDR_3:%.*]] = getelementptr inbounds i8, ptr [[MEM]], i64 [[IDX_3]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR_3]], align 4
; CHECK-NEXT:    [[INNER_COND_3:%.*]] = icmp ult i32 [[INNER_IV_NEXT_3]], 8
; CHECK-NEXT:    br i1 [[INNER_COND_3]], label [[INNER_HEADER_LATCH_EXITING_3]], label [[OUTER_LATCH_EXITING_3:%.*]], !llvm.loop [[LOOP0]]
; CHECK:       outer.latch_exiting.3:
; CHECK-NEXT:    ret void
;
entry:
  br label %outer.header

outer.header:                                                 ; preds = %entry, %outer.latch_exiting
  %outer.iv = phi i32 [ 0, %entry ], [ %outer.iv_next, %outer.latch_exiting ]
  br label %inner.header_latch_exiting

inner.header_latch_exiting:                                   ; preds = %outer.header, %inner.header_latch_exiting
  %inner.iv = phi i32 [ %outer.iv, %outer.header ], [ %inner.iv_next, %inner.header_latch_exiting ]
  %inner.iv_next = add nuw nsw i32 %inner.iv, 1
  %outer.iv.ext = zext nneg i32 %outer.iv to i64
  %idx_part = mul nuw nsw i64 %outer.iv.ext, 16
  %inner.iv.ext = zext nneg i32 %inner.iv to i64
  %idx = add nuw nsw i64 %idx_part, %inner.iv.ext
  %addr = getelementptr inbounds i8, ptr %mem, i64 %idx
  store i32 0, ptr %addr
  %inner.cond = icmp ult i32 %inner.iv_next, 8
  br i1 %inner.cond, label %inner.header_latch_exiting, label %outer.latch_exiting, !llvm.loop !1

outer.latch_exiting:                                          ; preds = %inner.header_latch_exiting
  %outer.iv_next = add nuw nsw i32 %outer.iv, 1
  %outer.cond = icmp ult i32 %outer.iv_next, 4
  br i1 %outer.cond, label %outer.header, label %end, !llvm.loop !1

end:                                                          ; preds = %outer.latch_exiting
  ret void
}

; Check that the outer loop of the same loop nest as dependent_sub_fullunroll
; is not fully unrolled when the inner loop's final IV value depends on a
; function argument instead of a combination of the outer IV and constants.

define void @dependent_sub_no_fullunroll(ptr noundef %mem, i32 noundef %inner.ub) {
; CHECK-LABEL: @dependent_sub_no_fullunroll(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[OUTER_HEADER:%.*]]
; CHECK:       outer.header:
; CHECK-NEXT:    [[OUTER_IV:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[OUTER_IV_NEXT_1:%.*]], [[OUTER_LATCH_EXITING_1:%.*]] ]
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING:%.*]]
; CHECK:       inner.header_latch_exiting:
; CHECK-NEXT:    [[INNER_IV:%.*]] = phi i32 [ [[OUTER_IV]], [[OUTER_HEADER]] ], [ [[INNER_IV_NEXT:%.*]], [[INNER_HEADER_LATCH_EXITING]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT]] = add nuw nsw i32 [[INNER_IV]], 1
; CHECK-NEXT:    [[OUTER_IV_EXT:%.*]] = zext nneg i32 [[OUTER_IV]] to i64
; CHECK-NEXT:    [[IDX_PART:%.*]] = mul nuw nsw i64 [[OUTER_IV_EXT]], 16
; CHECK-NEXT:    [[INNER_IV_EXT:%.*]] = zext nneg i32 [[INNER_IV]] to i64
; CHECK-NEXT:    [[IDX:%.*]] = add nuw nsw i64 [[IDX_PART]], [[INNER_IV_EXT]]
; CHECK-NEXT:    [[ADDR:%.*]] = getelementptr inbounds i8, ptr [[MEM:%.*]], i64 [[IDX]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR]], align 4
; CHECK-NEXT:    [[INNER_COND:%.*]] = icmp ult i32 [[INNER_IV_NEXT]], [[INNER_UB:%.*]]
; CHECK-NEXT:    br i1 [[INNER_COND]], label [[INNER_HEADER_LATCH_EXITING]], label [[OUTER_LATCH_EXITING:%.*]], !llvm.loop [[LOOP0]]
; CHECK:       outer.latch_exiting:
; CHECK-NEXT:    [[OUTER_IV_NEXT:%.*]] = add nuw nsw i32 [[OUTER_IV]], 1
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING_1:%.*]]
; CHECK:       inner.header_latch_exiting.1:
; CHECK-NEXT:    [[INNER_IV_1:%.*]] = phi i32 [ [[OUTER_IV_NEXT]], [[OUTER_LATCH_EXITING]] ], [ [[INNER_IV_NEXT_1:%.*]], [[INNER_HEADER_LATCH_EXITING_1]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT_1]] = add nuw nsw i32 [[INNER_IV_1]], 1
; CHECK-NEXT:    [[OUTER_IV_EXT_1:%.*]] = zext nneg i32 [[OUTER_IV_NEXT]] to i64
; CHECK-NEXT:    [[IDX_PART_1:%.*]] = mul nuw nsw i64 [[OUTER_IV_EXT_1]], 16
; CHECK-NEXT:    [[INNER_IV_EXT_1:%.*]] = zext nneg i32 [[INNER_IV_1]] to i64
; CHECK-NEXT:    [[IDX_1:%.*]] = add nuw nsw i64 [[IDX_PART_1]], [[INNER_IV_EXT_1]]
; CHECK-NEXT:    [[ADDR_1:%.*]] = getelementptr inbounds i8, ptr [[MEM]], i64 [[IDX_1]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR_1]], align 4
; CHECK-NEXT:    [[INNER_COND_1:%.*]] = icmp ult i32 [[INNER_IV_NEXT_1]], [[INNER_UB]]
; CHECK-NEXT:    br i1 [[INNER_COND_1]], label [[INNER_HEADER_LATCH_EXITING_1]], label [[OUTER_LATCH_EXITING_1]], !llvm.loop [[LOOP0]]
; CHECK:       outer.latch_exiting.1:
; CHECK-NEXT:    [[OUTER_IV_NEXT_1]] = add nuw nsw i32 [[OUTER_IV]], 2
; CHECK-NEXT:    [[OUTER_COND_1:%.*]] = icmp ult i32 [[OUTER_IV_NEXT_1]], 4
; CHECK-NEXT:    br i1 [[OUTER_COND_1]], label [[OUTER_HEADER]], label [[END:%.*]], !llvm.loop [[LOOP0]]
; CHECK:       end:
; CHECK-NEXT:    ret void
;
entry:
  br label %outer.header

outer.header:                                                 ; preds = %entry, %outer.latch_exiting
  %outer.iv = phi i32 [ 0, %entry ], [ %outer.iv_next, %outer.latch_exiting ]
  br label %inner.header_latch_exiting

inner.header_latch_exiting:                                   ; preds = %outer.header, %inner.header_latch_exiting
  %inner.iv = phi i32 [ %outer.iv, %outer.header ], [ %inner.iv_next, %inner.header_latch_exiting ]
  %inner.iv_next = add nuw nsw i32 %inner.iv, 1
  %outer.iv.ext = zext nneg i32 %outer.iv to i64
  %idx_part = mul nuw nsw i64 %outer.iv.ext, 16
  %inner.iv.ext = zext nneg i32 %inner.iv to i64
  %idx = add nuw nsw i64 %idx_part, %inner.iv.ext
  %addr = getelementptr inbounds i8, ptr %mem, i64 %idx
  store i32 0, ptr %addr
  %inner.cond = icmp ult i32 %inner.iv_next, %inner.ub
  br i1 %inner.cond, label %inner.header_latch_exiting, label %outer.latch_exiting, !llvm.loop !1

outer.latch_exiting:                                          ; preds = %inner.header_latch_exiting
  %outer.iv_next = add nuw nsw i32 %outer.iv, 1
  %outer.cond = icmp ult i32 %outer.iv_next, 4
  br i1 %outer.cond, label %outer.header, label %end, !llvm.loop !1

end:                                                          ; preds = %outer.latch_exiting
  ret void
}

; Make sure that the threshold bonus does not override a correctness check and
; unrolling when a convergent operation that is illegal to unroll is present.
; The loop nest is the same as before except for the fact that the outer
; loop's upper bound is now 11 (instead of 4) and there is an uncontrolled
; convergent call in the outer loop's header. Were the call non-convergent,
; the outer loop would be partially unrolled by a factor of 2, with a breakout
; of 1.

declare void @convergent_operation() convergent

define void @dont_unroll_illegal_convergent_op(ptr noundef %mem) {
; CHECK-LABEL: @dont_unroll_illegal_convergent_op(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[OUTER_HEADER:%.*]]
; CHECK:       outer.header:
; CHECK-NEXT:    [[OUTER_IV:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[OUTER_IV_NEXT:%.*]], [[OUTER_LATCH_EXITING:%.*]] ]
; CHECK-NEXT:    call void @convergent_operation()
; CHECK-NEXT:    br label [[INNER_HEADER_LATCH_EXITING:%.*]]
; CHECK:       inner.header_latch_exiting:
; CHECK-NEXT:    [[INNER_IV:%.*]] = phi i32 [ [[OUTER_IV]], [[OUTER_HEADER]] ], [ [[INNER_IV_NEXT:%.*]], [[INNER_HEADER_LATCH_EXITING]] ]
; CHECK-NEXT:    [[INNER_IV_NEXT]] = add nuw nsw i32 [[INNER_IV]], 1
; CHECK-NEXT:    [[OUTER_IV_EXT:%.*]] = zext nneg i32 [[OUTER_IV]] to i64
; CHECK-NEXT:    [[IDX_PART:%.*]] = mul nuw nsw i64 [[OUTER_IV_EXT]], 16
; CHECK-NEXT:    [[INNER_IV_EXT:%.*]] = zext nneg i32 [[INNER_IV]] to i64
; CHECK-NEXT:    [[IDX:%.*]] = add nuw nsw i64 [[IDX_PART]], [[INNER_IV_EXT]]
; CHECK-NEXT:    [[ADDR:%.*]] = getelementptr inbounds i8, ptr [[MEM:%.*]], i64 [[IDX]]
; CHECK-NEXT:    store i32 0, ptr [[ADDR]], align 4
; CHECK-NEXT:    [[INNER_COND:%.*]] = icmp ult i32 [[INNER_IV_NEXT]], 8
; CHECK-NEXT:    br i1 [[INNER_COND]], label [[INNER_HEADER_LATCH_EXITING]], label [[OUTER_LATCH_EXITING]], !llvm.loop [[LOOP0]]
; CHECK:       outer.latch_exiting:
; CHECK-NEXT:    [[OUTER_IV_NEXT]] = add nuw nsw i32 [[OUTER_IV]], 1
; CHECK-NEXT:    [[OUTER_COND:%.*]] = icmp ult i32 [[OUTER_IV_NEXT]], 11
; CHECK-NEXT:    br i1 [[OUTER_COND]], label [[OUTER_HEADER]], label [[END:%.*]], !llvm.loop [[LOOP0]]
; CHECK:       end:
; CHECK-NEXT:    ret void
;
entry:
  br label %outer.header

outer.header:                                                 ; preds = %entry, %outer.latch_exiting
  %outer.iv = phi i32 [ 0, %entry ], [ %outer.iv_next, %outer.latch_exiting ]
  call void @convergent_operation()
  br label %inner.header_latch_exiting

inner.header_latch_exiting:                                   ; preds = %outer.header, %inner.header_latch_exiting
  %inner.iv = phi i32 [ %outer.iv, %outer.header ], [ %inner.iv_next, %inner.header_latch_exiting ]
  %inner.iv_next = add nuw nsw i32 %inner.iv, 1
  %outer.iv.ext = zext nneg i32 %outer.iv to i64
  %idx_part = mul nuw nsw i64 %outer.iv.ext, 16
  %inner.iv.ext = zext nneg i32 %inner.iv to i64
  %idx = add nuw nsw i64 %idx_part, %inner.iv.ext
  %addr = getelementptr inbounds i8, ptr %mem, i64 %idx
  store i32 0, ptr %addr
  %inner.cond = icmp ult i32 %inner.iv_next, 8
  br i1 %inner.cond, label %inner.header_latch_exiting, label %outer.latch_exiting, !llvm.loop !1

outer.latch_exiting:                                          ; preds = %inner.header_latch_exiting
  %outer.iv_next = add nuw nsw i32 %outer.iv, 1
  %outer.cond = icmp ult i32 %outer.iv_next, 11
  br i1 %outer.cond, label %outer.header, label %end, !llvm.loop !1

end:                                                          ; preds = %outer.latch_exiting
  ret void
}

!1 = !{!1, !2}
!2 = !{!"amdgpu.loop.unroll.threshold", i32 100}
