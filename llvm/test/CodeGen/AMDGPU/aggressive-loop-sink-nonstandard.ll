; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx942 --aggressive-sink-insts-into-cycles=1 < %s | FileCheck -check-prefix=SUNK %s

; Check that various edge cases do not crash the compiler

; Multiple uses of sunk valu, chain of sink candidates

define half @global_agent_atomic_fmin_ret_f16__amdgpu_no_fine_grained_memory(ptr addrspace(1) %ptr, half %val) {
; SUNK-LABEL: global_agent_atomic_fmin_ret_f16__amdgpu_no_fine_grained_memory:
; SUNK:       ; %bb.0:
; SUNK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; SUNK-NEXT:    v_mov_b32_e32 v3, v0
; SUNK-NEXT:    v_and_b32_e32 v0, -4, v3
; SUNK-NEXT:    global_load_dword v4, v[0:1], off
; SUNK-NEXT:    v_and_b32_e32 v3, 3, v3
; SUNK-NEXT:    v_lshlrev_b32_e32 v3, 3, v3
; SUNK-NEXT:    s_mov_b32 s2, 0xffff
; SUNK-NEXT:    v_lshlrev_b32_e64 v5, v3, s2
; SUNK-NEXT:    s_mov_b64 s[0:1], 0
; SUNK-NEXT:    v_not_b32_e32 v5, v5
; SUNK-NEXT:    v_max_f16_e32 v2, v2, v2
; SUNK-NEXT:  .LBB0_1: ; %atomicrmw.start
; SUNK-NEXT:    ; =>This Inner Loop Header: Depth=1
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    v_mov_b32_e32 v7, v4
; SUNK-NEXT:    v_lshrrev_b32_e32 v4, v3, v7
; SUNK-NEXT:    v_max_f16_e32 v4, v4, v4
; SUNK-NEXT:    v_min_f16_e32 v4, v4, v2
; SUNK-NEXT:    v_lshlrev_b32_e32 v4, v3, v4
; SUNK-NEXT:    v_and_or_b32 v6, v7, v5, v4
; SUNK-NEXT:    buffer_wbl2 sc1
; SUNK-NEXT:    global_atomic_cmpswap v4, v[0:1], v[6:7], off sc0
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    buffer_inv sc1
; SUNK-NEXT:    v_cmp_eq_u32_e32 vcc, v4, v7
; SUNK-NEXT:    s_or_b64 s[0:1], vcc, s[0:1]
; SUNK-NEXT:    s_andn2_b64 exec, exec, s[0:1]
; SUNK-NEXT:    s_cbranch_execnz .LBB0_1
; SUNK-NEXT:  ; %bb.2: ; %atomicrmw.end
; SUNK-NEXT:    s_or_b64 exec, exec, s[0:1]
; SUNK-NEXT:    v_lshrrev_b32_e32 v0, v3, v4
; SUNK-NEXT:    s_setpc_b64 s[30:31]
  %result = atomicrmw fmin ptr addrspace(1) %ptr, half %val syncscope("agent") seq_cst
  ret half %result
}

; Sink candidates with multiple defs

define void @memmove_p5_p5(ptr addrspace(5) align 1 %dst, ptr addrspace(5) align 1 readonly %src, i64 %sz) {
; SUNK-LABEL: memmove_p5_p5:
; SUNK:       ; %bb.0: ; %entry
; SUNK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; SUNK-NEXT:    v_and_b32_e32 v4, 15, v2
; SUNK-NEXT:    v_mov_b32_e32 v5, 0
; SUNK-NEXT:    v_and_b32_e32 v6, -16, v2
; SUNK-NEXT:    v_mov_b32_e32 v7, v3
; SUNK-NEXT:    v_cmp_ne_u64_e64 s[0:1], 0, v[4:5]
; SUNK-NEXT:    v_cmp_ne_u64_e32 vcc, 0, v[6:7]
; SUNK-NEXT:    v_cmp_ge_u32_e64 s[2:3], v1, v0
; SUNK-NEXT:    s_and_saveexec_b64 s[4:5], s[2:3]
; SUNK-NEXT:    s_xor_b64 s[4:5], exec, s[4:5]
; SUNK-NEXT:    s_cbranch_execnz .LBB1_3
; SUNK-NEXT:  ; %bb.1: ; %Flow46
; SUNK-NEXT:    s_andn2_saveexec_b64 s[2:3], s[4:5]
; SUNK-NEXT:    s_cbranch_execnz .LBB1_10
; SUNK-NEXT:  .LBB1_2: ; %Flow47
; SUNK-NEXT:    s_or_b64 exec, exec, s[2:3]
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    s_setpc_b64 s[30:31]
; SUNK-NEXT:  .LBB1_3: ; %memmove_copy_forward
; SUNK-NEXT:    s_and_saveexec_b64 s[6:7], vcc
; SUNK-NEXT:    s_cbranch_execz .LBB1_6
; SUNK-NEXT:  ; %bb.4: ; %memmove_fwd_main_loop.preheader
; SUNK-NEXT:    s_mov_b64 s[8:9], 0
; SUNK-NEXT:    v_mov_b32_e32 v3, v1
; SUNK-NEXT:    v_mov_b32_e32 v8, v0
; SUNK-NEXT:  .LBB1_5: ; %memmove_fwd_main_loop
; SUNK-NEXT:    ; =>This Inner Loop Header: Depth=1
; SUNK-NEXT:    scratch_load_dwordx4 v[10:13], v3, off
; SUNK-NEXT:    v_lshl_add_u64 v[6:7], v[6:7], 0, -16
; SUNK-NEXT:    v_cmp_eq_u64_e64 s[2:3], 0, v[6:7]
; SUNK-NEXT:    v_add_u32_e32 v3, 16, v3
; SUNK-NEXT:    s_or_b64 s[8:9], s[2:3], s[8:9]
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    scratch_store_dwordx4 v8, v[10:13], off
; SUNK-NEXT:    v_add_u32_e32 v8, 16, v8
; SUNK-NEXT:    s_andn2_b64 exec, exec, s[8:9]
; SUNK-NEXT:    s_cbranch_execnz .LBB1_5
; SUNK-NEXT:  .LBB1_6: ; %Flow41
; SUNK-NEXT:    s_or_b64 exec, exec, s[6:7]
; SUNK-NEXT:    s_and_saveexec_b64 s[6:7], s[0:1]
; SUNK-NEXT:    s_cbranch_execz .LBB1_9
; SUNK-NEXT:  ; %bb.7: ; %memmove_fwd_residual_loop.preheader
; SUNK-NEXT:    v_and_b32_e32 v2, -16, v2
; SUNK-NEXT:    v_add_u32_e32 v0, v0, v2
; SUNK-NEXT:    v_add_u32_e32 v1, v1, v2
; SUNK-NEXT:    s_mov_b64 s[8:9], 0
; SUNK-NEXT:  .LBB1_8: ; %memmove_fwd_residual_loop
; SUNK-NEXT:    ; =>This Inner Loop Header: Depth=1
; SUNK-NEXT:    scratch_load_ubyte v2, v1, off
; SUNK-NEXT:    v_lshl_add_u64 v[4:5], v[4:5], 0, -1
; SUNK-NEXT:    v_cmp_eq_u64_e64 s[2:3], 0, v[4:5]
; SUNK-NEXT:    v_add_u32_e32 v1, 1, v1
; SUNK-NEXT:    s_or_b64 s[8:9], s[2:3], s[8:9]
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    scratch_store_byte v0, v2, off
; SUNK-NEXT:    v_add_u32_e32 v0, 1, v0
; SUNK-NEXT:    s_andn2_b64 exec, exec, s[8:9]
; SUNK-NEXT:    s_cbranch_execnz .LBB1_8
; SUNK-NEXT:  .LBB1_9: ; %Flow39
; SUNK-NEXT:    s_or_b64 exec, exec, s[6:7]
; SUNK-NEXT:    ; implicit-def: $vgpr2_vgpr3
; SUNK-NEXT:    ; implicit-def: $vgpr0
; SUNK-NEXT:    ; implicit-def: $vgpr1
; SUNK-NEXT:    ; implicit-def: $vgpr4_vgpr5
; SUNK-NEXT:    s_andn2_saveexec_b64 s[2:3], s[4:5]
; SUNK-NEXT:    s_cbranch_execz .LBB1_2
; SUNK-NEXT:  .LBB1_10: ; %memmove_copy_backwards
; SUNK-NEXT:    s_and_saveexec_b64 s[4:5], s[0:1]
; SUNK-NEXT:    s_cbranch_execz .LBB1_13
; SUNK-NEXT:  ; %bb.11: ; %memmove_bwd_residual_loop.preheader
; SUNK-NEXT:    v_add_u32_e32 v7, -1, v2
; SUNK-NEXT:    v_add_u32_e32 v6, v0, v7
; SUNK-NEXT:    v_add_u32_e32 v7, v1, v7
; SUNK-NEXT:    s_mov_b64 s[6:7], 0
; SUNK-NEXT:  .LBB1_12: ; %memmove_bwd_residual_loop
; SUNK-NEXT:    ; =>This Inner Loop Header: Depth=1
; SUNK-NEXT:    scratch_load_ubyte v8, v7, off
; SUNK-NEXT:    v_lshl_add_u64 v[4:5], v[4:5], 0, -1
; SUNK-NEXT:    v_cmp_eq_u64_e64 s[0:1], 0, v[4:5]
; SUNK-NEXT:    v_add_u32_e32 v7, -1, v7
; SUNK-NEXT:    s_or_b64 s[6:7], s[0:1], s[6:7]
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    scratch_store_byte v6, v8, off
; SUNK-NEXT:    v_add_u32_e32 v6, -1, v6
; SUNK-NEXT:    s_andn2_b64 exec, exec, s[6:7]
; SUNK-NEXT:    s_cbranch_execnz .LBB1_12
; SUNK-NEXT:  .LBB1_13: ; %Flow45
; SUNK-NEXT:    s_or_b64 exec, exec, s[4:5]
; SUNK-NEXT:    s_and_saveexec_b64 s[0:1], vcc
; SUNK-NEXT:    s_cbranch_execz .LBB1_16
; SUNK-NEXT:  ; %bb.14: ; %memmove_bwd_main_loop.preheader
; SUNK-NEXT:    v_and_b32_e32 v5, -16, v2
; SUNK-NEXT:    v_add_u32_e32 v4, -16, v5
; SUNK-NEXT:    v_add_u32_e32 v2, v0, v4
; SUNK-NEXT:    v_sub_co_u32_e32 v0, vcc, 0, v5
; SUNK-NEXT:    v_add_u32_e32 v4, v1, v4
; SUNK-NEXT:    s_mov_b64 s[4:5], 0
; SUNK-NEXT:    v_subb_co_u32_e32 v1, vcc, 0, v3, vcc
; SUNK-NEXT:  .LBB1_15: ; %memmove_bwd_main_loop
; SUNK-NEXT:    ; =>This Inner Loop Header: Depth=1
; SUNK-NEXT:    scratch_load_dwordx4 v[6:9], v4, off
; SUNK-NEXT:    v_lshl_add_u64 v[0:1], v[0:1], 0, 16
; SUNK-NEXT:    v_cmp_eq_u64_e32 vcc, 0, v[0:1]
; SUNK-NEXT:    v_add_u32_e32 v4, -16, v4
; SUNK-NEXT:    s_or_b64 s[4:5], vcc, s[4:5]
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    scratch_store_dwordx4 v2, v[6:9], off
; SUNK-NEXT:    v_add_u32_e32 v2, -16, v2
; SUNK-NEXT:    s_andn2_b64 exec, exec, s[4:5]
; SUNK-NEXT:    s_cbranch_execnz .LBB1_15
; SUNK-NEXT:  .LBB1_16: ; %Flow43
; SUNK-NEXT:    s_or_b64 exec, exec, s[0:1]
; SUNK-NEXT:    s_or_b64 exec, exec, s[2:3]
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    s_setpc_b64 s[30:31]
entry:
  tail call void @llvm.memmove.p5.p5.i64(ptr addrspace(5) noundef nonnull align 1 %dst, ptr addrspace(5) noundef nonnull align 1 %src, i64 %sz, i1 false)
  ret void
}

; We should not sink the mfma into the if/else as it is convergent

define void @convergent_sink(<4 x i16> %in0, <4 x i16> %in1, i32 %val, i32 %v, ptr addrspace(1) %outptr) #2 {
; SUNK-LABEL: convergent_sink:
; SUNK:       ; %bb.0: ; %entry
; SUNK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; SUNK-NEXT:    v_mfma_f32_32x32x8_bf16 a[0:15], v[0:1], v[2:3], 0
; SUNK-NEXT:    v_lshl_add_u32 v0, v5, 1, v5
; SUNK-NEXT:    v_lshlrev_b32_e32 v2, 1, v5
; SUNK-NEXT:    s_mov_b32 s4, 0
; SUNK-NEXT:    s_mov_b64 s[0:1], 0
; SUNK-NEXT:    v_mov_b32_e32 v5, 0xde
; SUNK-NEXT:    v_ashrrev_i32_e32 v1, 31, v0
; SUNK-NEXT:    v_ashrrev_i32_e32 v3, 31, v2
; SUNK-NEXT:    s_branch .LBB2_2
; SUNK-NEXT:  .LBB2_1: ; %end
; SUNK-NEXT:    ; in Loop: Header=BB2_2 Depth=1
; SUNK-NEXT:    v_cmp_eq_u32_e32 vcc, v8, v4
; SUNK-NEXT:    s_add_i32 s4, s4, 1
; SUNK-NEXT:    s_or_b64 s[0:1], vcc, s[0:1]
; SUNK-NEXT:    s_andn2_b64 exec, exec, s[0:1]
; SUNK-NEXT:    s_cbranch_execz .LBB2_7
; SUNK-NEXT:  .LBB2_2: ; %loop.body
; SUNK-NEXT:    ; =>This Inner Loop Header: Depth=1
; SUNK-NEXT:    s_cmp_lt_i32 s4, 6
; SUNK-NEXT:    global_store_dword v[6:7], v5, off
; SUNK-NEXT:    s_cbranch_scc0 .LBB2_4
; SUNK-NEXT:  ; %bb.3: ; %else
; SUNK-NEXT:    ; in Loop: Header=BB2_2 Depth=1
; SUNK-NEXT:    v_lshl_add_u64 v[8:9], v[0:1], 3, v[6:7]
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[12:15], off offset:48
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[8:11], off offset:32
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[4:7], off offset:16
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[0:3], off
; SUNK-NEXT:    s_mov_b64 s[2:3], 0
; SUNK-NEXT:    s_branch .LBB2_5
; SUNK-NEXT:  .LBB2_4: ; in Loop: Header=BB2_2 Depth=1
; SUNK-NEXT:    s_mov_b64 s[2:3], -1
; SUNK-NEXT:  .LBB2_5: ; %Flow
; SUNK-NEXT:    ; in Loop: Header=BB2_2 Depth=1
; SUNK-NEXT:    s_andn2_b64 vcc, exec, s[2:3]
; SUNK-NEXT:    v_mov_b32_e32 v8, v0
; SUNK-NEXT:    s_cbranch_vccnz .LBB2_1
; SUNK-NEXT:  ; %bb.6: ; %if
; SUNK-NEXT:    ; in Loop: Header=BB2_2 Depth=1
; SUNK-NEXT:    v_lshl_add_u64 v[8:9], v[2:3], 3, v[6:7]
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[12:15], off offset:48
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[8:11], off offset:32
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[4:7], off offset:16
; SUNK-NEXT:    global_store_dwordx4 v[8:9], a[0:3], off
; SUNK-NEXT:    v_mov_b32_e32 v8, v2
; SUNK-NEXT:    s_branch .LBB2_1
; SUNK-NEXT:  .LBB2_7: ; %exit
; SUNK-NEXT:    s_or_b64 exec, exec, s[0:1]
; SUNK-NEXT:    global_store_dwordx4 v[6:7], a[12:15], off offset:48
; SUNK-NEXT:    global_store_dwordx4 v[6:7], a[8:11], off offset:32
; SUNK-NEXT:    global_store_dwordx4 v[6:7], a[4:7], off offset:16
; SUNK-NEXT:    global_store_dwordx4 v[6:7], a[0:3], off
; SUNK-NEXT:    s_waitcnt vmcnt(0)
; SUNK-NEXT:    s_setpc_b64 s[30:31]
entry:
  %1005 = tail call <16 x float> @llvm.amdgcn.mfma.f32.32x32x8bf16.1k(<4 x i16> %in0, <4 x i16> %in1, <16 x float> zeroinitializer, i32 0, i32 0, i32 0)
  br label %loop.body

loop.body:
  %i = phi i32 [0, %entry], [%i.inc, %end]
  store i32 222, ptr addrspace(1) %outptr
  %cc = icmp sgt i32 %i, 5
  br i1 %cc, label %if, label %else

if:
  %v.if = mul i32 %v, 2
  %sptr.if =  getelementptr <4 x i16>, ptr addrspace(1) %outptr, i32 %v.if
  store <16 x float> %1005, ptr addrspace(1) %sptr.if
  br label %end

else:
  %v.else = mul i32 %v, 3
  %sptr.else =  getelementptr <4 x i16>, ptr addrspace(1) %outptr, i32 %v.else
  store <16 x float> %1005, ptr addrspace(1) %sptr.else
  br label %end

end:
  %r = phi i32 [ %v.if, %if ], [ %v.else, %else ]
  %cmp = icmp ne i32 %r, %val
  %i.inc = add i32 %i, 1
  br i1 %cmp, label %loop.body, label %exit

exit:
  store <16 x float> %1005, ptr addrspace(1) %outptr
  ret void
}
