; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt --amdgpu-annotate-varying-branch-weights -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx942 < %s | FileCheck %s

declare i32 @llvm.amdgcn.workitem.id.x()
declare i32 @llvm.amdgcn.workitem.id.y()

; The branch here is likely varying:
define amdgpu_kernel void @cond_store_even(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR1:[0-9]+]] !reqd_work_group_size [[META0:![0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1:![0-9]+]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely varying:
define amdgpu_kernel void @cond_store_even_ann_cf(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ann_cf(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR1]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[CF_VAL:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]])
; CHECK-NEXT:    [[PSEUDO_COND:%.*]] = extractvalue { i1, i64 } [[CF_VAL]], 0
; CHECK-NEXT:    [[MASK:%.*]] = extractvalue { i1, i64 } [[CF_VAL]], 1
; CHECK-NEXT:    br i1 [[PSEUDO_COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[MASK]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  %cf_val = call { i1, i64 } @llvm.amdgcn.if.i64(i1 %cond)
  %pseudo.cond = extractvalue { i1, i64 } %cf_val, 0
  %mask = extractvalue { i1, i64 } %cf_val, 1
  br i1 %pseudo.cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  call void @llvm.amdgcn.end.cf.i64(i64 %mask)
  ret void
}


; The branch here is likely varying:
define amdgpu_kernel void @cond_store_complex1(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_complex1(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR1]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:    [[TID_X:%.*]] = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[K:%.*]] = lshr i32 [[TID]], 4
; CHECK-NEXT:    [[COND:%.*]] = icmp ult i32 [[K]], 15
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
  %tid.x = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %k = lshr i32 %tid, 4
  %cond = icmp ult i32 %k, 15
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely varying:
define amdgpu_kernel void @cond_store_complex2(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_complex2(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR1]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:    [[TID_X:%.*]] = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[J:%.*]] = and i32 [[TID]], 15
; CHECK-NEXT:    [[COND:%.*]] = icmp ult i32 [[J]], 15
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
  %tid.x = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %j = and i32 %tid, 15
  %cond = icmp ult i32 %j, 15
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely varying:
define amdgpu_kernel void @cond_store_even_only_reqd_wgsz(ptr addrspace(1) inreg %dest) !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_only_reqd_wgsz(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2:[0-9]+]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely varying:
define amdgpu_kernel void @cond_store_even_only_flat_wgsz(ptr addrspace(1) inreg %dest) #0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_only_flat_wgsz(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely varying, since the y dimension varies in each
; wavefront with the required work group size:
define amdgpu_kernel void @cond_store_even_ydim_small_wgs(ptr addrspace(1) inreg %dest) !reqd_work_group_size !1 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ydim_small_wgs(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META2:![0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 3
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID_Y]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]], !prof [[PROF1]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 3
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid.y, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is not likely varying, because there are no attributes with
; work group size information:
define amdgpu_kernel void @cond_store_even_no_attributes(ptr addrspace(1) inreg %dest) {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_no_attributes(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is not likely varying, because the condition only depends on a
; workitem id dimension that does not vary per wavefront (namely y):
define amdgpu_kernel void @cond_store_even_ydim(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ydim(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR1]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID_Y]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid.y, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is not likely varying, because its condition is directly
; loaded from memory:
define amdgpu_kernel void @cond_store_loaded(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_loaded(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]]) #[[ATTR1]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LOOKUP_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[LOOKUP]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[LOOKUP_VALUE:%.*]] = load i32, ptr addrspace(1) [[LOOKUP_ADDR]], align 4
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LOOKUP_VALUE]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lookup.addr = getelementptr i32, ptr addrspace(1) %lookup, i64 %tid.ext
  %lookup.value = load i32, ptr addrspace(1) %lookup.addr
  %cond = icmp eq i32 %lookup.value, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is not likely varying, because its condition directly results from a PHI:
define amdgpu_kernel void @cond_store_loop_phi(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup, i32 %n) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_loop_phi(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]], i32 [[N:%.*]]) #[[ATTR1]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[VAL:%.*]] = phi i32 [ [[VAL_INC:%.*]], %[[LOOP]] ], [ [[TID]], %[[ENTRY]] ]
; CHECK-NEXT:    [[IDX:%.*]] = phi i32 [ [[IDX_DEC:%.*]], %[[LOOP]] ], [ [[N]], %[[ENTRY]] ]
; CHECK-NEXT:    [[VAL_INC]] = add i32 [[VAL]], 1
; CHECK-NEXT:    [[IDX_DEC]] = sub i32 [[IDX]], 1
; CHECK-NEXT:    [[LOOP_COND:%.*]] = icmp eq i32 [[IDX_DEC]], 0
; CHECK-NEXT:    br i1 [[LOOP_COND]], label %[[LOOP_END:.*]], label %[[LOOP]]
; CHECK:       [[LOOP_END]]:
; CHECK-NEXT:    [[LOOKUP_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[LOOKUP]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[LOOKUP_VALUE:%.*]] = load i32, ptr addrspace(1) [[LOOKUP_ADDR]], align 4
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LOOKUP_VALUE]], 0
; CHECK-NEXT:    br i1 [[COND]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  br label %loop
loop:
  %val = phi i32 [%val.inc, %loop], [%tid, %entry]
  %idx = phi i32 [%idx.dec, %loop], [%n, %entry]
  %val.inc = add i32 %val, 1
  %idx.dec = sub i32 %idx, 1
  %loop.cond = icmp eq i32 %idx.dec, 0
  br i1 %loop.cond, label %loop.end, label %loop
loop.end:
  %lookup.addr = getelementptr i32, ptr addrspace(1) %lookup, i64 %tid.ext
  %lookup.value = load i32, ptr addrspace(1) %lookup.addr
  %cond = icmp eq i32 %lookup.value, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


attributes #0 = {"amdgpu-flat-work-group-size"="256,256"}
!0 = !{i32 64, i32 4, i32 1}
!1 = !{i32 8, i32 32, i32 1}
;.
; CHECK: [[META0]] = !{i32 64, i32 4, i32 1}
; CHECK: [[PROF1]] = !{!"branch_weights", i32 2126008812, i32 21474835}
; CHECK: [[META2]] = !{i32 8, i32 32, i32 1}
;.
