# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
# RUN: llc -mtriple=riscv64 -mattr=+v,+m -run-pass=instruction-select -simplify-mir -verify-machineinstrs %s -o - | FileCheck  %s

---
name:            splat_zero_nxv1i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv1i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF8_:%[0-9]+]]:vr = PseudoVMV_V_X_MF8 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 1 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 1 x s8>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv2i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv2i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF4_:%[0-9]+]]:vr = PseudoVMV_V_X_MF4 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 2 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 2 x s8>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv4i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv4i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF2_:%[0-9]+]]:vr = PseudoVMV_V_X_MF2 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 4 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 4 x s8>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv8i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv8i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M1_:%[0-9]+]]:vr = PseudoVMV_V_X_M1 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_M1_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 8 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 8 x s8>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv16i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv16i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M2_:%[0-9]+]]:vrm2 = PseudoVMV_V_X_M2 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m2 = COPY [[PseudoVMV_V_X_M2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m2
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 16 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8m2 = COPY %0(<vscale x 16 x s8>)
    PseudoRET implicit $v8m2

...
---
name:            splat_zero_nxv32i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv32i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M4_:%[0-9]+]]:vrm4 = PseudoVMV_V_X_M4 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m4 = COPY [[PseudoVMV_V_X_M4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m4
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 32 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8m4 = COPY %0(<vscale x 32 x s8>)
    PseudoRET implicit $v8m4

...
---
name:            splat_zero_nxv64i8
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv64i8
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M8_:%[0-9]+]]:vrm8 = PseudoVMV_V_X_M8 [[DEF]], [[COPY]], -1, 3 /* e8 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m8 = COPY [[PseudoVMV_V_X_M8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 64 x s8>) = G_SPLAT_VECTOR %2(s64)
    $v8m8 = COPY %0(<vscale x 64 x s8>)
    PseudoRET implicit $v8m8

...
---
name:            splat_zero_nxv1i16
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv1i16
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF4_:%[0-9]+]]:vr = PseudoVMV_V_X_MF4 [[DEF]], [[COPY]], -1, 4 /* e16 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 1 x s16>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 1 x s16>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv2i16
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv2i16
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF2_:%[0-9]+]]:vr = PseudoVMV_V_X_MF2 [[DEF]], [[COPY]], -1, 4 /* e16 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 2 x s16>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 2 x s16>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv4i16
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv4i16
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M1_:%[0-9]+]]:vr = PseudoVMV_V_X_M1 [[DEF]], [[COPY]], -1, 4 /* e16 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_M1_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 4 x s16>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 4 x s16>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv8i16
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv8i16
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M2_:%[0-9]+]]:vrm2 = PseudoVMV_V_X_M2 [[DEF]], [[COPY]], -1, 4 /* e16 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m2 = COPY [[PseudoVMV_V_X_M2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m2
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 8 x s16>) = G_SPLAT_VECTOR %2(s64)
    $v8m2 = COPY %0(<vscale x 8 x s16>)
    PseudoRET implicit $v8m2

...
---
name:            splat_zero_nxv16i16
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv16i16
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M4_:%[0-9]+]]:vrm4 = PseudoVMV_V_X_M4 [[DEF]], [[COPY]], -1, 4 /* e16 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m4 = COPY [[PseudoVMV_V_X_M4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m4
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 16 x s16>) = G_SPLAT_VECTOR %2(s64)
    $v8m4 = COPY %0(<vscale x 16 x s16>)
    PseudoRET implicit $v8m4

...
---
name:            splat_zero_nxv32i16
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv32i16
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M8_:%[0-9]+]]:vrm8 = PseudoVMV_V_X_M8 [[DEF]], [[COPY]], -1, 4 /* e16 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m8 = COPY [[PseudoVMV_V_X_M8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m8
    %3:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 32 x s16>) = G_SPLAT_VECTOR %2(s64)
    $v8m8 = COPY %0(<vscale x 32 x s16>)
    PseudoRET implicit $v8m8

...
---
name:            splat_zero_nxv1i32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv1i32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF2_:%[0-9]+]]:vr = PseudoVMV_V_X_MF2 [[DEF]], [[COPY]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %1:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %1(s32)
    %0:vrb(<vscale x 1 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 1 x s32>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv2i32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv2i32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M1_:%[0-9]+]]:vr = PseudoVMV_V_X_M1 [[DEF]], [[COPY]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_M1_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %1:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %1(s32)
    %0:vrb(<vscale x 2 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 2 x s32>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv4i32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv4i32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M2_:%[0-9]+]]:vrm2 = PseudoVMV_V_X_M2 [[DEF]], [[COPY]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m2 = COPY [[PseudoVMV_V_X_M2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m2
    %1:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %1(s32)
    %0:vrb(<vscale x 4 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8m2 = COPY %0(<vscale x 4 x s32>)
    PseudoRET implicit $v8m2

...
---
name:            splat_zero_nxv8i32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv8i32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M4_:%[0-9]+]]:vrm4 = PseudoVMV_V_X_M4 [[DEF]], [[COPY]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m4 = COPY [[PseudoVMV_V_X_M4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m4
    %1:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %1(s32)
    %0:vrb(<vscale x 8 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8m4 = COPY %0(<vscale x 8 x s32>)
    PseudoRET implicit $v8m4

...
---
name:            splat_zero_nxv16i32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv16i32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M8_:%[0-9]+]]:vrm8 = PseudoVMV_V_X_M8 [[DEF]], [[COPY]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m8 = COPY [[PseudoVMV_V_X_M8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m8
    %1:gprb(s32) = G_CONSTANT i32 0
    %2:gprb(s64) = G_ANYEXT %1(s32)
    %0:vrb(<vscale x 16 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8m8 = COPY %0(<vscale x 16 x s32>)
    PseudoRET implicit $v8m8

...
---
name:            splat_zero_nxv1i64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv1i64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M1_:%[0-9]+]]:vr = PseudoVMV_V_X_M1 [[DEF]], [[COPY]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_M1_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %1:gprb(s64) = G_CONSTANT i64 0
    %0:vrb(<vscale x 1 x s64>) = G_SPLAT_VECTOR %1(s64)
    $v8 = COPY %0(<vscale x 1 x s64>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv2i64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv2i64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M2_:%[0-9]+]]:vrm2 = PseudoVMV_V_X_M2 [[DEF]], [[COPY]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m2 = COPY [[PseudoVMV_V_X_M2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m2
    %1:gprb(s64) = G_CONSTANT i64 0
    %0:vrb(<vscale x 2 x s64>) = G_SPLAT_VECTOR %1(s64)
    $v8m2 = COPY %0(<vscale x 2 x s64>)
    PseudoRET implicit $v8m2

...
---
name:            splat_zero_nxv4i64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv4i64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M4_:%[0-9]+]]:vrm4 = PseudoVMV_V_X_M4 [[DEF]], [[COPY]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m4 = COPY [[PseudoVMV_V_X_M4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m4
    %1:gprb(s64) = G_CONSTANT i64 0
    %0:vrb(<vscale x 4 x s64>) = G_SPLAT_VECTOR %1(s64)
    $v8m4 = COPY %0(<vscale x 4 x s64>)
    PseudoRET implicit $v8m4

...
---
name:            splat_zero_nxv8i64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv8i64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M8_:%[0-9]+]]:vrm8 = PseudoVMV_V_X_M8 [[DEF]], [[COPY]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m8 = COPY [[PseudoVMV_V_X_M8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m8
    %1:gprb(s64) = G_CONSTANT i64 0
    %0:vrb(<vscale x 8 x s64>) = G_SPLAT_VECTOR %1(s64)
    $v8m8 = COPY %0(<vscale x 8 x s64>)
    PseudoRET implicit $v8m8

...
---
name:            splat_zero_nxv1f32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv1f32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_W_X:%[0-9]+]]:fpr32 = FMV_W_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_W_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_MF2_:%[0-9]+]]:vr = PseudoVMV_V_X_MF2 [[DEF]], [[COPY1]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_MF2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %1:fprb(s32) = G_FCONSTANT float 0.000000e+00
    %3:gprb(s32) = COPY %1(s32)
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 1 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 1 x s32>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv2f32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv2f32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_W_X:%[0-9]+]]:fpr32 = FMV_W_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_W_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M1_:%[0-9]+]]:vr = PseudoVMV_V_X_M1 [[DEF]], [[COPY1]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_M1_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %1:fprb(s32) = G_FCONSTANT float 0.000000e+00
    %3:gprb(s32) = COPY %1(s32)
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 2 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 2 x s32>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv4f32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv4f32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_W_X:%[0-9]+]]:fpr32 = FMV_W_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_W_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M2_:%[0-9]+]]:vrm2 = PseudoVMV_V_X_M2 [[DEF]], [[COPY1]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m2 = COPY [[PseudoVMV_V_X_M2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m2
    %1:fprb(s32) = G_FCONSTANT float 0.000000e+00
    %3:gprb(s32) = COPY %1(s32)
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 4 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8m2 = COPY %0(<vscale x 4 x s32>)
    PseudoRET implicit $v8m2

...
---
name:            splat_zero_nxv8f32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv8f32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_W_X:%[0-9]+]]:fpr32 = FMV_W_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_W_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M4_:%[0-9]+]]:vrm4 = PseudoVMV_V_X_M4 [[DEF]], [[COPY1]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m4 = COPY [[PseudoVMV_V_X_M4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m4
    %1:fprb(s32) = G_FCONSTANT float 0.000000e+00
    %3:gprb(s32) = COPY %1(s32)
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 8 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8m4 = COPY %0(<vscale x 8 x s32>)
    PseudoRET implicit $v8m4

...
---
name:            splat_zero_nxv16f32
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv16f32
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_W_X:%[0-9]+]]:fpr32 = FMV_W_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_W_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M8_:%[0-9]+]]:vrm8 = PseudoVMV_V_X_M8 [[DEF]], [[COPY1]], -1, 5 /* e32 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m8 = COPY [[PseudoVMV_V_X_M8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m8
    %1:fprb(s32) = G_FCONSTANT float 0.000000e+00
    %3:gprb(s32) = COPY %1(s32)
    %2:gprb(s64) = G_ANYEXT %3(s32)
    %0:vrb(<vscale x 16 x s32>) = G_SPLAT_VECTOR %2(s64)
    $v8m8 = COPY %0(<vscale x 16 x s32>)
    PseudoRET implicit $v8m8

...
---
name:            splat_zero_nxv1f64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv1f64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_D_X:%[0-9]+]]:fpr64 = FMV_D_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_D_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M1_:%[0-9]+]]:vr = PseudoVMV_V_X_M1 [[DEF]], [[COPY1]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8 = COPY [[PseudoVMV_V_X_M1_]]
    ; CHECK-NEXT: PseudoRET implicit $v8
    %1:fprb(s64) = G_FCONSTANT double 0.000000e+00
    %2:gprb(s64) = COPY %1(s64)
    %0:vrb(<vscale x 1 x s64>) = G_SPLAT_VECTOR %2(s64)
    $v8 = COPY %0(<vscale x 1 x s64>)
    PseudoRET implicit $v8

...
---
name:            splat_zero_nxv2f64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv2f64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_D_X:%[0-9]+]]:fpr64 = FMV_D_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_D_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M2_:%[0-9]+]]:vrm2 = PseudoVMV_V_X_M2 [[DEF]], [[COPY1]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m2 = COPY [[PseudoVMV_V_X_M2_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m2
    %1:fprb(s64) = G_FCONSTANT double 0.000000e+00
    %2:gprb(s64) = COPY %1(s64)
    %0:vrb(<vscale x 2 x s64>) = G_SPLAT_VECTOR %2(s64)
    $v8m2 = COPY %0(<vscale x 2 x s64>)
    PseudoRET implicit $v8m2

...
---
name:            splat_zero_nxvf64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxvf64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_D_X:%[0-9]+]]:fpr64 = FMV_D_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_D_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M4_:%[0-9]+]]:vrm4 = PseudoVMV_V_X_M4 [[DEF]], [[COPY1]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m4 = COPY [[PseudoVMV_V_X_M4_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m4
    %1:fprb(s64) = G_FCONSTANT double 0.000000e+00
    %2:gprb(s64) = COPY %1(s64)
    %0:vrb(<vscale x 4 x s64>) = G_SPLAT_VECTOR %2(s64)
    $v8m4 = COPY %0(<vscale x 4 x s64>)
    PseudoRET implicit $v8m4

...
---
name:            splat_zero_nxv8f64
legalized:       true
regBankSelected: true
body:             |
  bb.1:
    ; CHECK-LABEL: name: splat_zero_nxv8f64
    ; CHECK: [[COPY:%[0-9]+]]:gpr = COPY $x0
    ; CHECK-NEXT: [[FMV_D_X:%[0-9]+]]:fpr64 = FMV_D_X [[COPY]]
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gpr = COPY [[FMV_D_X]]
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
    ; CHECK-NEXT: [[ADDI:%[0-9]+]]:gpr = ADDI $x0, -1
    ; CHECK-NEXT: [[PseudoVMV_V_X_M8_:%[0-9]+]]:vrm8 = PseudoVMV_V_X_M8 [[DEF]], [[COPY1]], -1, 6 /* e64 */, 0 /* tu, mu */
    ; CHECK-NEXT: $v8m8 = COPY [[PseudoVMV_V_X_M8_]]
    ; CHECK-NEXT: PseudoRET implicit $v8m8
    %1:fprb(s64) = G_FCONSTANT double 0.000000e+00
    %2:gprb(s64) = COPY %1(s64)
    %0:vrb(<vscale x 8 x s64>) = G_SPLAT_VECTOR %2(s64)
    $v8m8 = COPY %0(<vscale x 8 x s64>)
    PseudoRET implicit $v8m8

...
